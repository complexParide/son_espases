{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897df461",
   "metadata": {},
   "source": [
    "# Notebook to predict patient arrivals at HUSE\n",
    "This jupyter notebook contains all the code used in the paper \"Forecasting emergency department visits in the reference hospital of the Balearic Islands: the role of tourist and weather data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3af6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python modules (Python version 3.11.4)\n",
    "\n",
    "import numpy as np  # version 1.25.2\n",
    "import pandas as pd  # version 2.1.1\n",
    "import matplotlib.pyplot as plt  # version 3.7.1\n",
    "from matplotlib import cm  # version 3.7.1\n",
    "from datetime import datetime, timedelta  # version 3.11.4 (python base module)\n",
    "import pickle  # version 3.11.4\n",
    "import os  # version 3.11.4\n",
    "import warnings  # version 3.11.4\n",
    "from tqdm import tqdm  # version 4.65.0\n",
    "from workalendar.europe import Spain  # version 17.0.0\n",
    "from scipy.stats import multivariate_normal, norm  # version 1.11.1\n",
    "from itertools import combinations, combinations_with_replacement # version 3.11.4\n",
    "from sklearn.preprocessing import StandardScaler  # version 1.2.2\n",
    "from sklearn.model_selection import train_test_split  # version 1.2.2\n",
    "from sklearn.ensemble import RandomForestRegressor  # version 1.2.2\n",
    "from sklearn.svm import SVR  # version 1.2.2\n",
    "import torch  # version 2.0.1\n",
    "import torch.nn as nn  # version 2.0.1\n",
    "from dieboldmariano import dm_test  # version 1.1.0\n",
    "from statsmodels.tsa.arima.model import ARIMA  # version 0.14.0\n",
    "\n",
    "# Setting matplotlib graphic theme\n",
    "plt.style.use('bmh')\n",
    "\n",
    "# Defining time variables\n",
    "ZERO_DATE = datetime(year=2015, month=12, day=26)\n",
    "COVZ, COVE = 1527, 2197\n",
    "COVID_ZERO = ZERO_DATE + pd.Timedelta(days=COVZ)\n",
    "COVID_END = ZERO_DATE + pd.Timedelta(days=COVE)\n",
    "END_STEP = 2562\n",
    "\n",
    "# Setting the random seed\n",
    "SEED = 14041999\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Defining age cohorts and sex\n",
    "AGE_COHORTS = [0, 15, 25, 35, 45, 55, 65, 75, 85, np.inf]\n",
    "AGE_LABELS = ['0-15', *[f\"{i}-{i+10}\" for i in range(15, 85, 10)], '85+']\n",
    "GENDER_DICT = {'M': 'male', 'F': 'female', 'N': 'other'}\n",
    "\n",
    "# Defining shifts and custom holidays\n",
    "DAYTIMES = ['morning', 'afternoon', 'night']\n",
    "PALMADAYS = [[6, 1], [20, 1], [1, 3], [1, 5], [15, 8],\n",
    "             [12, 10], [1, 11], [6, 12], [8, 12], [26, 12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b06523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the datasets\n",
    "# They are defined UPPERCASE to distinguish them from the locally defined df\n",
    "\n",
    "X_TRAIN = pd.read_csv('X_train.csv')\n",
    "Y_TRAIN = pd.read_csv('Y_train.csv')\n",
    "X_VALID = pd.read_csv('X_validation.csv')\n",
    "Y_VALID = pd.read_csv('Y_validation.csv')\n",
    "X_TEST = pd.read_csv('X_test.csv')\n",
    "Y_TEST = pd.read_csv('Y_test.csv')\n",
    "X_COVID = pd.read_csv('X_post-covid.csv')\n",
    "Y_COVID = pd.read_csv('Y_post-covid.csv')\n",
    "\n",
    "print('Datasets uploaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the week-month NIP plots\n",
    "def weeklyEmergency():\n",
    "    weeklabels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    monthlabels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    lstys = ['--', '--', '--', '--', '--', '-', '-']\n",
    "    maks = ['o', 's', '^', 'd', 'v', 'o', 's']\n",
    "    ylims = {'morning': [120, 210], 'afternoon': [80, 145], 'night': [50, 100]}\n",
    "    XDF = pd.concat((X_TRAIN, X_VALID, X_TEST, X_COVID))\n",
    "    YDF = pd.concat((Y_TRAIN, Y_VALID, Y_TEST, Y_COVID))\n",
    "    for dt in DAYTIMES:\n",
    "        figa, ax = plt.subplots(1, 1, figsize=(9, 7))\n",
    "        for w in range(7):\n",
    "            T = XDF[XDF['weekday'] == w]['timestep'].values\n",
    "            months = [(ZERO_DATE + timedelta(days=t)).month for t in T]\n",
    "            Y = YDF[XDF['weekday'] == w][f\"total_{dt}\"]\n",
    "            xmonths = np.arange(1, 13)\n",
    "            ymonths = np.zeros(12)\n",
    "            for x in xmonths:\n",
    "                ymonths[x-1] = np.mean(Y[months == x])\n",
    "            ax.scatter(xmonths, ymonths, color=cm.Spectral_r(w/6), marker=maks[w],\n",
    "                       label=weeklabels[w], linewidth=0.5, edgecolor='black',\n",
    "                       s=100, zorder=24)\n",
    "            ax.plot(xmonths, ymonths, color=cm.Spectral_r(w/6),\n",
    "                    linestyle=lstys[w], zorder=12)\n",
    "        ax.set_xticks(xmonths)\n",
    "        ax.set_xticklabels(monthlabels, fontsize=14)\n",
    "        ax.set_yticklabels(ax.get_yticks().astype(int), fontsize=14)\n",
    "        ax.set_title(f\"Average NIP per specific weekday/month ({dt} shift)\", fontsize=16)\n",
    "        ax.set_xlim(0.75, 12.25)\n",
    "        ax.set_ylim(*ylims[dt])\n",
    "        legend = ax.legend(loc='upper center', fontsize=13.5, ncol=7, columnspacing=0.5)\n",
    "        legend.set_zorder(50)\n",
    "        plt.savefig(f\"weeks-vs-months_{dt}.pdf\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close(figa)\n",
    "            \n",
    "            \n",
    "weeklyEmergency()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21da19a",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc702579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance function\n",
    "def perforMeter(pred_y, true_y, shifts=DAYTIMES):\n",
    "    diff = pred_y - true_y\n",
    "    mae = np.mean(np.abs(diff), axis=0)\n",
    "    rmse = np.sqrt(np.mean((diff) ** 2, axis=0))\n",
    "    mape = 200 * np.mean(np.abs(diff) / (np.abs(pred_y) + np.abs(true_y)), axis=0)\n",
    "    dicto = {'mape': {}, 'rmse': {}, 'mae': {}}\n",
    "    for i, dt in enumerate(shifts):\n",
    "        dicto['mape'][dt] = mape[i]\n",
    "        dicto['rmse'][dt] = rmse[i]\n",
    "        dicto['mae'][dt] = mae[i]\n",
    "    return dicto\n",
    "\n",
    "\n",
    "# Hyperparameter tuning: Random Forests\n",
    "def treeTuner(X_train, X_valid, Y_train, Y_valid, ntrees, niters=100):\n",
    "    columns = ['n_trees', 'shift', 'mape', 'mape_sd', 'rmse', 'rmse_sd', 'mae', 'mae_sd']\n",
    "    Y_train = Y_train.values\n",
    "    X_tr = X_train.values\n",
    "    X_te = X_valid.values\n",
    "    normies = [StandardScaler() for _ in range(X_tr.shape[1])]\n",
    "    for i in range(len(normies)):\n",
    "        normies[i].fit(X_tr[:, i].reshape(-1, 1))\n",
    "        X_tr[:, i] = normies[i].transform(X_tr[:, i].reshape(-1, 1)).flatten()\n",
    "        X_te[:, i] = normies[i].transform(X_te[:, i].reshape(-1, 1)).flatten()\n",
    "    df = []\n",
    "    for nt in ntrees:\n",
    "        subdict = {'mape': dict(zip(DAYTIMES, [[], [], []])),\n",
    "                   'rmse': dict(zip(DAYTIMES, [[], [], []])),\n",
    "                   'mae': dict(zip(DAYTIMES, [[], [], []]))}\n",
    "        woods = RandomForestRegressor(n_estimators=nt, n_jobs=-1)\n",
    "        woods.fit(X_tr, Y_train)\n",
    "        full_preds = woods.predict(X_te)\n",
    "        for ni in tqdm(range(niters), colour='green', ncols=111, desc=f\"{nt}\",\n",
    "                       position=0, leave=True):\n",
    "            if ni % 10 == 0:\n",
    "                np.random.seed(SEED + ni // 10)\n",
    "            vidx = np.random.randint(low=0, high=X_valid.shape[0], size=X_valid.shape[0])\n",
    "            tree_preds = full_preds.copy()[vidx]\n",
    "            Y_test = Y_valid.values.copy()[vidx]\n",
    "            dicto = perforMeter(tree_preds, Y_test)\n",
    "            for met in subdict:\n",
    "                for dt in dicto[met]:\n",
    "                    subdict[met][dt].append(dicto[met][dt])\n",
    "        for dt in DAYTIMES:\n",
    "            line = [nt, dt]\n",
    "            for meth in subdict:\n",
    "                line.append(np.mean(subdict[meth][dt]))\n",
    "                line.append(np.std(subdict[meth][dt]))\n",
    "            df.append(line)\n",
    "    df = pd.DataFrame(df, columns=columns)\n",
    "    df.to_csv(f\"tuning-df_rf.csv\", index=None)\n",
    "    np.random.seed(SEED)\n",
    "    display(df)\n",
    "\n",
    "\n",
    "ycols = ['total_morning', 'total_afternoon', 'total_night']\n",
    "tree_axis = np.arange(25, 201, 25)\n",
    "treeTuner(X_TRAIN, X_VALID, Y_TRAIN[ycols], Y_VALID[ycols], tree_axis, niters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4637a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance function\n",
    "def perforMeter(pred_y, true_y, shifts=DAYTIMES):\n",
    "    diff = pred_y - true_y\n",
    "    mae = np.mean(np.abs(diff), axis=0)\n",
    "    rmse = np.sqrt(np.mean((diff) ** 2, axis=0))\n",
    "    mape = 200 * np.mean(np.abs(diff) / (np.abs(pred_y) + np.abs(true_y)), axis=0)\n",
    "    dicto = {'mape': {}, 'rmse': {}, 'mae': {}}\n",
    "    for i, dt in enumerate(shifts):\n",
    "        dicto['mape'][dt] = mape[i]\n",
    "        dicto['rmse'][dt] = rmse[i]\n",
    "        dicto['mae'][dt] = mae[i]\n",
    "    return dicto\n",
    "\n",
    "\n",
    "# Hyperparameter tuning: Support Vector Regressor\n",
    "def vecTuner(X_train, X_valid, Y_train, Y_valid, degrees, niters=100):\n",
    "    columns = ['degree', 'shift', 'mape', 'mape_sd', 'rmse', 'rmse_sd', 'mae', 'mae_sd']\n",
    "    Y_train = Y_train.values\n",
    "    df = []\n",
    "    X_tr = X_train.values\n",
    "    X_te = X_valid.values\n",
    "    normies = [StandardScaler() for _ in range(X_tr.shape[1])]\n",
    "    for i in range(len(normies)):\n",
    "        normies[i].fit(X_tr[:, i].reshape(-1, 1))\n",
    "        X_tr[:, i] = normies[i].transform(X_tr[:, i].reshape(-1, 1)).flatten()\n",
    "        X_te[:, i] = normies[i].transform(X_te[:, i].reshape(-1, 1)).flatten()\n",
    "    for nt in degrees:\n",
    "        subdict = {'mape': dict(zip(DAYTIMES, [[], [], []])),\n",
    "                   'rmse': dict(zip(DAYTIMES, [[], [], []])),\n",
    "                   'mae': dict(zip(DAYTIMES, [[], [], []]))}\n",
    "        svr = [SVR(degree=nt) for _ in range(3)]\n",
    "        full_preds = []\n",
    "        for i in range(3):\n",
    "            svr[i].fit(X_tr, Y_train[:, i])\n",
    "            full_preds.append(svr[i].predict(X_te))\n",
    "        full_preds = np.array(full_preds).T\n",
    "        for ni in tqdm(range(niters), colour='green', ncols=111, desc=f\"{nt}\",\n",
    "                       position=0, leave=True):\n",
    "            if ni % 10 == 0:\n",
    "                np.random.seed(SEED + ni // 10)\n",
    "            vidx = np.random.randint(low=0, high=X_valid.shape[0], size=X_valid.shape[0])\n",
    "            svr_preds = full_preds.copy()[vidx]\n",
    "            Y_test = Y_valid.values[vidx]\n",
    "            dicto = perforMeter(svr_preds, Y_test)\n",
    "            for met in subdict:\n",
    "                for dt in dicto[met]:\n",
    "                    subdict[met][dt].append(dicto[met][dt])\n",
    "        for dt in DAYTIMES:\n",
    "            line = [nt, dt]\n",
    "            for meth in subdict:\n",
    "                line.append(np.mean(subdict[meth][dt]))\n",
    "                line.append(np.std(subdict[meth][dt]))\n",
    "            df.append(line)\n",
    "    df = pd.DataFrame(df, columns=columns)\n",
    "    df.to_csv(f\"tuning-df_svr.csv\", index=None)\n",
    "    np.random.seed(SEED)\n",
    "    display(df)\n",
    "\n",
    "\n",
    "ycols = ['total_morning', 'total_afternoon', 'total_night']\n",
    "deg_axis = np.arange(1, 11)\n",
    "vecTuner(X_TRAIN, X_VALID, Y_TRAIN[ycols], Y_VALID[ycols], deg_axis, niters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results of hyperparameter tuning for RF and SVR\n",
    "def simpleTuPlotter(chosen_rf, chosen_svr, sigmas=1):\n",
    "    tdf = pd.read_csv(f\"tuning-df_rf.csv\")\n",
    "    vdf = pd.read_csv(f\"tuning-df_svr.csv\")\n",
    "    colors = dict(zip(DAYTIMES, ['xkcd:kermit green', 'xkcd:scarlet', 'xkcd:sapphire']))\n",
    "    figa, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    for shift in DAYTIMES:\n",
    "        stdf = tdf[tdf['shift'] == shift]\n",
    "        svdf = vdf[vdf['shift'] == shift]\n",
    "        ax[0].plot(stdf['n_trees'], stdf['mape'], marker='o', color=colors[shift],\n",
    "                   label=shift, zorder=10)\n",
    "        ax[0].fill_between(stdf['n_trees'], stdf['mape'] + sigmas * stdf['mape_sd'],\n",
    "                           stdf['mape'] - sigmas * stdf['mape_sd'], color=colors[shift],\n",
    "                           zorder=9, alpha=0.25)\n",
    "        ax[1].plot(svdf['degree'], svdf['mape'], marker='o', color=colors[shift],\n",
    "                   label=shift, zorder=10)\n",
    "        ax[1].fill_between(svdf['degree'], svdf['mape'] + sigmas * svdf['mape_sd'],\n",
    "                           svdf['mape'] - sigmas * svdf['mape_sd'], color=colors[shift],\n",
    "                           zorder=9, alpha=0.25)\n",
    "    ax[0].axvline(chosen_rf, linestyle='--', color='xkcd:steel gray')\n",
    "    ax[1].axvline(chosen_svr, linestyle='--', color='xkcd:steel gray')\n",
    "    ax[0].set_xticks(range(25, 201, 25))\n",
    "    ax[1].set_xticks(range(1, 11))\n",
    "    ax[0].set_xlabel('Number of trees', fontsize=12)\n",
    "    ax[1].set_xlabel('Degree', fontsize=12)\n",
    "    ax[0].set_title('Random Forest')\n",
    "    ax[1].set_title('Support Vector Regressor')\n",
    "    for i in range(2):\n",
    "        ax[i].legend(loc='upper right', fontsize=12)\n",
    "        ax[i].set_ylabel('MAPE on validation dataset', fontsize=12)\n",
    "        ax[i].set_yticks(range(0, 25, 1))\n",
    "        ax[i].set_ylim(6, 20)\n",
    "    plt.savefig(f\"tuning-plot_rf-svf.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "simpleTuPlotter(100, 3, sigmas=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fbbfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning: Feedforward Neural Network\n",
    "def deepTuner(X_train, X_valid, Y_train, Y_valid, nepochs, niters=100):\n",
    "    L, VL = [], []\n",
    "    X_tr = X_train.values\n",
    "    y = torch.Tensor(Y_train.values)\n",
    "    normies = [StandardScaler() for _ in range(X_tr.shape[1])]\n",
    "    for i in range(len(normies)):\n",
    "        normies[i].fit(X_tr[:, i].reshape(-1, 1))\n",
    "        X_tr[:, i] = normies[i].transform(X_tr[:, i].reshape(-1, 1)).flatten()\n",
    "    X = torch.Tensor(X_tr)\n",
    "    crisafully_cnet = nn.Sequential(nn.Linear(X.shape[1], 32), nn.ReLU(),\n",
    "                                    nn.Linear(32, 64), nn.ReLU(),\n",
    "                                    nn.Linear(64, 128), nn.ReLU(),\n",
    "                                    nn.Linear(128, 256), nn.ReLU(),\n",
    "                                    nn.Linear(256, 512), nn.ReLU(),\n",
    "                                    nn.Linear(512, 1024), nn.ReLU(),\n",
    "                                    nn.Linear(1024, 2048), nn.ReLU(),\n",
    "                                    nn.Linear(2048, 1024), nn.ReLU(),\n",
    "                                    nn.Linear(1024, 512), nn.ReLU(),\n",
    "                                    nn.Linear(512, 256), nn.ReLU(),\n",
    "                                    nn.Linear(256, 128), nn.ReLU(),\n",
    "                                    nn.Linear(128, 64), nn.ReLU(),\n",
    "                                    nn.Linear(64, 32), nn.ReLU(),\n",
    "                                    nn.Linear(32, y.shape[1]))\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(crisafully_cnet.parameters(), lr=0.01)\n",
    "    for epoch in tqdm(range(nepochs), colour='green', ncols=111,\n",
    "               position=0, leave=True):\n",
    "        vlosses = []\n",
    "        optimizer.zero_grad()\n",
    "        output = crisafully_cnet(X)\n",
    "        loss = criterion(output, y)\n",
    "        L.append(loss.item())\n",
    "        with torch.no_grad():\n",
    "            for ni in range(niters):\n",
    "                if ni % 10 == 0:\n",
    "                    np.random.seed(SEED + ni // 10)\n",
    "                vidx = np.random.randint(low=0, high=X_valid.shape[0], size=X_valid.shape[0])\n",
    "                X_te = X_valid.values.copy()[vidx]\n",
    "                for i in range(len(normies)):\n",
    "                    X_te[:, i] = normies[i].transform(X_te[:, i].reshape(-1, 1)).flatten()\n",
    "                Xv = torch.Tensor(X_te)\n",
    "                yv = torch.Tensor(Y_valid.values.copy()[vidx])\n",
    "                output = crisafully_cnet(Xv)\n",
    "                vloss = criterion(output, yv)\n",
    "                vlosses.append(vloss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        VL.append(vlosses)\n",
    "    np.random.seed(SEED)\n",
    "    pickle.dump({'L': np.array(L), 'VL': np.array(VL)},\n",
    "                open(\"tuning-dict_fnn.pickle\", 'wb'))\n",
    "\n",
    "\n",
    "ycols = ['total_morning', 'total_afternoon', 'total_night']\n",
    "deepTuner(X_TRAIN, X_VALID, Y_TRAIN[ycols], Y_VALID[ycols], 500, niters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0acd8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results of hyperparameter tuning for FNN\n",
    "def deepTuPlotter(L, VL, chosen, ylim=None, xlim=None):\n",
    "    losses, vlosses = L, np.mean(VL, axis=1)\n",
    "    slosses = np.std(VL, axis=1)\n",
    "    nepochs = len(losses)\n",
    "    if xlim is None:\n",
    "        xlim = nepochs\n",
    "    figa, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    ax.plot(range(nepochs), losses, zorder=10,\n",
    "            label='training set', color='xkcd:sapphire')\n",
    "    ax.fill_between(range(nepochs), vlosses + slosses, vlosses - slosses,\n",
    "                    color='xkcd:scarlet', alpha=0.5, zorder=1)\n",
    "    ax.plot(range(nepochs), vlosses, zorder=9,\n",
    "            label='validation set', color='xkcd:scarlet')\n",
    "    ax.axvline(chosen, linestyle='--', color='xkcd:steel gray')\n",
    "    ax.set_ylim(0, ylim)\n",
    "    ax.set_xlim(0, xlim)\n",
    "    ax.set_xlabel('Number of epochs', fontsize=14)\n",
    "    ax.set_ylabel('Loss function', fontsize=14)\n",
    "    ax.legend(loc='upper right', fontsize=12)\n",
    "    plt.savefig(f\"tuning-plot_fnn.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "loss_dict = pickle.load(open(\"tuning-dict_fnn.pickle\", 'rb'))\n",
    "deepTuPlotter(loss_dict['L'], loss_dict['VL'], chosen=130, ylim=1000, xlim=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be171783",
   "metadata": {},
   "source": [
    "### Predictions for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions for the test dataset for SARIMA, RF, SVR, and FNN\n",
    "def theGreatPredictor(X_train, X_test, Y_train, Y_test, Y_valid, ntrees, degree, nepochs,\n",
    "                      niters=100, aged=False):\n",
    "    T = {}\n",
    "    wp_cols = ['timestep', 'weekday', 'yearday', 'holiday_-2',\n",
    "               'holiday_-1', 'holiday_0', 'holiday_+1', 'holiday_+2',\n",
    "               'temp_min', 'temp_max', 'prec_prob', 'wind_speed',\n",
    "               'resident_pop', 'tourist_pop']\n",
    "    wx_cols = ['timestep', 'weekday', 'yearday', 'holiday_-2',\n",
    "               'holiday_-1', 'holiday_0', 'holiday_+1', 'holiday_+2',\n",
    "               'temp_min', 'temp_max', 'prec_prob', 'wind_speed', 'resident_pop']\n",
    "    xp_cols = ['timestep', 'weekday', 'yearday', 'holiday_-2',\n",
    "               'holiday_-1', 'holiday_0', 'holiday_+1', 'holiday_+2',\n",
    "               'resident_pop', 'tourist_pop']\n",
    "    xx_cols = ['timestep', 'weekday', 'yearday', 'holiday_-2',\n",
    "               'holiday_-1', 'holiday_0', 'holiday_+1', 'holiday_+2', 'resident_pop']\n",
    "    if aged:\n",
    "        daytimes = ['low', 'medium', 'high']\n",
    "        prefix = 'risk'\n",
    "    else:\n",
    "        daytimes = DAYTIMES\n",
    "        prefix = 'shift'\n",
    "    # SARIMA\n",
    "    Y_train, Y_valid, Y_test = Y_train.values, Y_valid.values, Y_test\n",
    "    Y_a1 = np.zeros((len(Y_test), 3))\n",
    "    Y_a3 = np.zeros((len(Y_test), 3))\n",
    "    Y_a7 = np.zeros((len(Y_test), 3))\n",
    "    Y_a14 = np.zeros((len(Y_test), 3))\n",
    "    for t in tqdm(np.arange(-14, len(Y_test)-1), colour='red', ncols=111, desc='SARIMA'):\n",
    "        if t < -1:\n",
    "            Y = Y_valid[:t+1]\n",
    "        elif t == -1:\n",
    "            Y = Y_valid\n",
    "        else:\n",
    "            Y = np.row_stack((Y_valid[t+1:], Y_test[:t+1]))\n",
    "        with warnings.catch_warnings(action=\"ignore\"):\n",
    "            arima = [ARIMA(Y[:, i], order=(1, 1, 1), seasonal_order=(1, 1, 1, 7),\n",
    "                           ).fit() for i in range(3)]\n",
    "        if t < len(Y_test) - 14:\n",
    "            apred = np.array([arima[i].forecast(steps=14) for i in range(3)])\n",
    "            if t > -2:\n",
    "                Y_a1[t+1] = apred[:, 0]\n",
    "            if t > -4:\n",
    "                Y_a3[t+3] = apred[:, 2]\n",
    "            if t > -8:\n",
    "                Y_a7[t+7] = apred[:, 6]\n",
    "            Y_a14[t+14] = apred[:, 13]\n",
    "        if t < len(Y_test) - 7:\n",
    "            apred = np.array([arima[i].forecast(steps=7) for i in range(3)])\n",
    "            Y_a1[t+1] = apred[:, 0]\n",
    "            Y_a3[t+3] = apred[:, 2]\n",
    "            Y_a7[t+7] = apred[:, 6]\n",
    "        elif t < len(Y_test) - 3:\n",
    "            apred = np.array([arima[i].forecast(steps=3) for i in range(3)])\n",
    "            Y_a1[t+1] = apred[:, 0]\n",
    "            Y_a3[t+3] = apred[:, 2]\n",
    "        else:\n",
    "            apred = np.array([arima[i].forecast(steps=1)[0] for i in range(3)])\n",
    "            Y_a1[t+1] = apred\n",
    "    T['SARIMA-1'] = dict(zip(daytimes, np.array([Y_a1[:, t] for t in range(3)])))\n",
    "    T['SARIMA-3'] = dict(zip(daytimes, np.array([Y_a3[:, t] for t in range(3)])))\n",
    "    T['SARIMA-7'] = dict(zip(daytimes, np.array([Y_a7[:, t] for t in range(3)])))\n",
    "    T['SARIMA-14'] = dict(zip(daytimes, np.array([Y_a14[:, t] for t in range(3)])))\n",
    "    # RF, SVR, and FNN\n",
    "    X_tr = {'CWP': X_train[wp_cols].copy().values, 'CWX': X_train[wx_cols].copy().values,\n",
    "            'CXP': X_train[xp_cols].copy().values, 'CXX': X_train[xx_cols].copy().values}\n",
    "    T['RF'] = {}\n",
    "    T['SVR'] = {}\n",
    "    T['FNN'] = {}\n",
    "    normies, woods, svr = {}, {}, {}\n",
    "    y = torch.Tensor(Y_train)\n",
    "    for k, cols in zip(X_tr, [wp_cols, wx_cols, xp_cols, xx_cols]):\n",
    "        normies[k] = [StandardScaler() for _ in range(X_tr[k].shape[1])]\n",
    "        for i in range(len(normies[k])):\n",
    "            normies[k][i].fit(X_tr[k][:, i].reshape(-1, 1))\n",
    "            X_tr[k][:, i] = normies[k][i].transform(X_tr[k][:, i].reshape(-1, 1)).flatten()\n",
    "        woods[k] = RandomForestRegressor(n_estimators=ntrees, n_jobs=-1) \n",
    "        woods[k].fit(X_tr[k], Y_train)\n",
    "        svr[k] = [SVR(degree=degree) for _ in range(3)]\n",
    "        for i in range(3):\n",
    "            svr[k][i].fit(X_tr[k], Y_train[:, i])\n",
    "        X = torch.Tensor(X_tr[k])\n",
    "        crisafully_cnet = nn.Sequential(nn.Linear(X.shape[1], 32), nn.ReLU(),\n",
    "                                        nn.Linear(32, 64), nn.ReLU(),\n",
    "                                        nn.Linear(64, 128), nn.ReLU(),\n",
    "                                        nn.Linear(128, 256), nn.ReLU(),\n",
    "                                        nn.Linear(256, 512), nn.ReLU(),\n",
    "                                        nn.Linear(512, 1024), nn.ReLU(),\n",
    "                                        nn.Linear(1024, 2048), nn.ReLU(),\n",
    "                                        nn.Linear(2048, 1024), nn.ReLU(),\n",
    "                                        nn.Linear(1024, 512), nn.ReLU(),\n",
    "                                        nn.Linear(512, 256), nn.ReLU(),\n",
    "                                        nn.Linear(256, 128), nn.ReLU(),\n",
    "                                        nn.Linear(128, 64), nn.ReLU(),\n",
    "                                        nn.Linear(64, 32), nn.ReLU(),\n",
    "                                        nn.Linear(32, y.shape[1]))\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(crisafully_cnet.parameters(), lr=0.01)\n",
    "        for epoch in tqdm(range(nepochs), colour='blue', ncols=111,\n",
    "                          position=0, leave=True, desc=f'FNN training ({k})'):\n",
    "            optimizer.zero_grad()\n",
    "            output = crisafully_cnet(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        T['RF'][k] = {}\n",
    "        T['SVR'][k] = {}\n",
    "        T['FNN'][k] = {}\n",
    "        with torch.no_grad():\n",
    "            X_te = X_test[cols].copy().values\n",
    "            for i in range(len(normies[k])):\n",
    "                X_te[:, i] = normies[k][i].transform(X_te[:, i].reshape(-1, 1)).flatten()\n",
    "            tree_preds = woods[k].predict(X_te)\n",
    "            Xv = torch.Tensor(X_te)\n",
    "            deep_preds = crisafully_cnet(Xv).numpy()\n",
    "            for t, dt in enumerate(daytimes):\n",
    "                T['RF'][k][dt] = tree_preds[:, t]\n",
    "                T['SVR'][k][dt] = svr[k][t].predict(X_te)\n",
    "                T['FNN'][k][dt] = deep_preds[:, t]\n",
    "    # Bootstrap procedure\n",
    "    D = {'Y': dict(zip(daytimes, [[], [], []])),\n",
    "         'SARIMA-1': dict(zip(daytimes, [[], [], []])),\n",
    "         'SARIMA-3': dict(zip(daytimes, [[], [], []])),\n",
    "         'SARIMA-7': dict(zip(daytimes, [[], [], []])),\n",
    "         'SARIMA-14': dict(zip(daytimes, [[], [], []])),\n",
    "         'RF': dict(zip(X_tr, [dict(zip(daytimes, [[], [], []])) for _ in range(4)])),\n",
    "         'SVR': dict(zip(X_tr, [dict(zip(daytimes, [[], [], []])) for _ in range(4)])),\n",
    "         'FNN': dict(zip(X_tr, [dict(zip(daytimes, [[], [], []])) for _ in range(4)]))}\n",
    "    for ni in tqdm(range(niters), colour='red', ncols=111, desc='Bootstrapping'):\n",
    "        if ni % 10 == 0:\n",
    "            np.random.seed(SEED + ni // 10)\n",
    "        vidx = np.random.randint(low=0, high=X_test.shape[0], size=X_test.shape[0])\n",
    "        for t, dt in enumerate(daytimes):\n",
    "            D['Y'][dt].append(Y_test.values[vidx, t])\n",
    "            D['SARIMA-1'][dt].append(T['SARIMA-1'][dt][vidx])\n",
    "            D['SARIMA-3'][dt].append(T['SARIMA-3'][dt][vidx])\n",
    "            D['SARIMA-7'][dt].append(T['SARIMA-7'][dt][vidx])\n",
    "            D['SARIMA-14'][dt].append(T['SARIMA-14'][dt][vidx])\n",
    "            for meth in ['RF', 'SVR', 'FNN']:\n",
    "                for k in X_tr:\n",
    "                    D[meth][k][dt].append(T[meth][k][dt][vidx])\n",
    "    for t, dt in enumerate(daytimes):\n",
    "        for meth in ['Y', 'SARIMA-1', 'SARIMA-3', 'SARIMA-7', 'SARIMA-14']:\n",
    "            D[meth][dt] = np.array(D[meth][dt])\n",
    "        for meth in ['RF', 'SVR', 'FNN']:\n",
    "            for k in X_tr:\n",
    "                D[meth][k][dt] = np.array(D[meth][k][dt])\n",
    "    pickle.dump(D, open(f\"great-predict_{prefix}.pickle\", 'wb'))\n",
    "    \n",
    "\n",
    "# shift-based predictions\n",
    "ycols = ['total_morning', 'total_afternoon', 'total_night']\n",
    "theGreatPredictor(X_TRAIN, X_TEST, Y_TRAIN[ycols], Y_TEST[ycols], Y_VALID[ycols],\n",
    "                  ntrees=100, degree=3, nepochs=130, niters=1000, aged=False)\n",
    "\n",
    "# risk-group-based predictions\n",
    "ycols = ['total_low', 'total_medium', 'total_high']\n",
    "theGreatPredictor(X_TRAIN, X_TEST, Y_TRAIN[ycols], Y_TEST[ycols], Y_VALID[ycols],\n",
    "                  ntrees=100, degree=3, nepochs=130, niters=1000, aged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3bb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance function\n",
    "def perforMeter(pred_y, true_y, shifts=DAYTIMES):\n",
    "    dicto = {'mape': {}, 'mape_std': {}, 'rmse': {},\n",
    "             'rmse_std': {}, 'mae': {}, 'mae_std': {}}\n",
    "    for t in shifts:\n",
    "        diff = pred_y[t] - true_y[t]\n",
    "        mape = 200 * np.mean(np.abs(diff) / (np.abs(pred_y[t]) + np.abs(true_y[t])), axis=1)\n",
    "        rmse = np.sqrt(np.mean((diff) ** 2, axis=1))\n",
    "        mae = np.mean(np.abs(diff), axis=1)\n",
    "        dicto['mape'][t] = np.mean(mape)\n",
    "        dicto['mape_std'][t] = np.std(mape)\n",
    "        dicto['rmse'][t] = np.mean(rmse)\n",
    "        dicto['rmse_std'][t] = np.std(rmse)\n",
    "        dicto['mae'][t] = np.mean(mae)\n",
    "        dicto['mae_std'][t] = np.std(mae)\n",
    "    return dicto\n",
    "\n",
    "\n",
    "# Generating supplementary tables\n",
    "def resultFunction(GD, aged=False):\n",
    "    if aged:\n",
    "        daytimes = ['low', 'medium', 'high']\n",
    "        prefix = 'risk'\n",
    "    else:\n",
    "        daytimes = DAYTIMES\n",
    "        prefix = 'shift'\n",
    "    niters = len(GD['SARIMA-1'])\n",
    "    metrics = ['mape', 'rmse', 'mae']\n",
    "    inpudict = {'CWP': 'All', 'CXP': 'No W', 'CWX': 'No T', 'CXX': 'No W No T'}\n",
    "    columns = ['method', 'input', 'shift', 'mape', 'mape_std',\n",
    "               'rmse', 'rmse_std', 'mae', 'mae_std']\n",
    "    # performance table\n",
    "    df = []\n",
    "    for meth in ['SARIMA-1', 'SARIMA-7', 'SARIMA-14']:\n",
    "        dicto = perforMeter(GD[meth], GD['Y'], shifts=daytimes)\n",
    "        for t in daytimes:\n",
    "            df.append([meth, '', t.capitalize(), dicto['mape'][t], dicto['mape_std'][t],\n",
    "                       dicto['rmse'][t], dicto['rmse_std'][t], dicto['mae'][t],\n",
    "                       dicto['mae_std'][t]])\n",
    "    for meth in ['RF', 'SVR', 'FNN']:\n",
    "        for ind in inpudict:\n",
    "            dicto = perforMeter(GD[meth][ind], GD['Y'], shifts=daytimes)\n",
    "            for t in daytimes:\n",
    "                df.append([meth, inpudict[ind], t.capitalize(), dicto['mape'][t],\n",
    "                           dicto['mape_std'][t], dicto['rmse'][t], dicto['rmse_std'][t],\n",
    "                           dicto['mae'][t], dicto['mae_std'][t]])\n",
    "    df = pd.DataFrame(df, columns=columns)\n",
    "    df.to_csv(f\"metric-table-full_{prefix}.csv\", index=None)\n",
    "    display(df)\n",
    "\n",
    "\n",
    "# Generating main tables\n",
    "def toLatex():\n",
    "    sdf = pd.read_csv(f\"metric-table-full_shift.csv\")\n",
    "    rdf = pd.read_csv(f\"metric-table-full_risk.csv\")\n",
    "    for metric in ['mape', 'rmse', 'mae']:\n",
    "        columns = ['method', 'input', f'{metric}_morning', f'{metric}_afternoon',\n",
    "                   f'{metric}_night', f'{metric}_low', f'{metric}_medium', f'{metric}_high']\n",
    "        df = [['SARIMA-1', float('nan'), *sdf[metric].iloc[:3], *rdf[metric].iloc[:3]],\n",
    "              ['SARIMA-7', float('nan'), *sdf[metric].iloc[3:6], *rdf[metric].iloc[3:6]]]\n",
    "        for i in range(6, 40, 3):\n",
    "            df.append([sdf.iloc[i]['method'], sdf.iloc[i]['input'],\n",
    "                       *sdf.iloc[i:i+3][metric], *rdf.iloc[i:i+3][metric]])\n",
    "        df = pd.DataFrame(df, columns=columns)\n",
    "        df.to_csv(f\"metric-table-latex_{metric}.csv\", index=None)\n",
    "        display(df)\n",
    "\n",
    "\n",
    "gdict = pickle.load(open(f\"great-predict_shift.pickle\", 'rb'))\n",
    "resultFunction(gdict, aged=False)\n",
    "\n",
    "gdict = pickle.load(open(f\"great-predict_risk.pickle\", 'rb'))\n",
    "resultFunction(gdict, aged=True)\n",
    "\n",
    "toLatex()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243cf96a",
   "metadata": {},
   "source": [
    "### Diebold-Mariano tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313dd78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diebold-Mariano fraction function\n",
    "def dieboFunc(pred_a, pred_b, y_test, shifts=DAYTIMES, thresh=0.05, n_tests=1):\n",
    "    diebolds = {}\n",
    "    corr = thresh / n_tests\n",
    "    for t in shifts:\n",
    "        diebolds[t] = [dm_test(y_test[t][ni], pred_a[t][ni], pred_b[t][ni])[1]\n",
    "                       for ni in range(y_test[t].shape[0])]\n",
    "        diebolds[t] = [dm > corr for dm in diebolds[t]]\n",
    "        diebolds[t] = np.sum(diebolds[t]) / len(diebolds[t])\n",
    "    return diebolds\n",
    "\n",
    "\n",
    "# Diebold-Mariano table generator\n",
    "def dieBolder(GD, aged=False, n_tests=1):\n",
    "    if aged:\n",
    "        daytimes = ['low', 'medium', 'high']\n",
    "        prefix = 'risk'\n",
    "    else:\n",
    "        daytimes = DAYTIMES\n",
    "        prefix = 'shift'\n",
    "    inpudict = {'CWP': 'All', 'CXP': 'No W', 'CWX': 'No T', 'CXX': 'No W T'}\n",
    "    # diebold table\n",
    "    dbar = tqdm(total=n_tests, colour='yellow', ncols=111,\n",
    "                position=0, leave=True, desc=f'die-{prefix}')\n",
    "    df = []\n",
    "    columns = ['method_1', 'input_1', 'method_2', 'input_2', 'shift', 'equiv_frac']\n",
    "    for m1, m2 in combinations(['SARIMA-1', 'SARIMA-7', 'SARIMA-14'], 2):\n",
    "        diebold = dieboFunc(GD[m1], GD[m2], GD['Y'],\n",
    "                            shifts=daytimes, thresh=0.05, n_tests=n_tests)\n",
    "        dbar.update(3)\n",
    "        for t in daytimes:\n",
    "            df.append([m1, '', m2, '', t.capitalize(), diebold[t]])\n",
    "    for bmeth in ['SARIMA-1', 'SARIMA-7', 'SARIMA-14']:\n",
    "        for meth in ['RF', 'SVR', 'FNN']:\n",
    "            for ind in inpudict:\n",
    "                diebold = dieboFunc(GD[bmeth], GD[meth][ind], GD['Y'],\n",
    "                                    shifts=daytimes, thresh=0.05, n_tests=n_tests)\n",
    "                dbar.update(3)\n",
    "                for t in daytimes:\n",
    "                    df.append([bmeth, '', meth, inpudict[ind], t.capitalize(), diebold[t]])\n",
    "    for m1, m2 in combinations_with_replacement(['RF', 'SVR', 'FNN'], 2):\n",
    "        for v1, v2 in combinations_with_replacement(['CWP', 'CWX', 'CXP', 'CXX'], 2):\n",
    "            if m1 == m2 and v1 == v2:\n",
    "                continue\n",
    "            diebold = dieboFunc(GD[m1][v1], GD[m2][v2], GD['Y'],\n",
    "                                shifts=daytimes, thresh=0.05, n_tests=n_tests)\n",
    "            dbar.update(3)\n",
    "            for t in daytimes:\n",
    "                df.append([m1, inpudict[v1], m2, inpudict[v2], t.capitalize(), diebold[t]])\n",
    "            if m1 != m2 and v1 != v2:\n",
    "                diebold = dieboFunc(GD[m1][v2], GD[m2][v1], GD['Y'],\n",
    "                                    shifts=daytimes, thresh=0.05, n_tests=n_tests)\n",
    "                dbar.update(3)\n",
    "                for t in daytimes:\n",
    "                    df.append([m1, inpudict[v2], m2, inpudict[v1],\n",
    "                               t.capitalize(), diebold[t]])\n",
    "    df = pd.DataFrame(df, columns=columns)\n",
    "    df.to_csv(f\"diebold-table_{prefix}.csv\", index=None)\n",
    "    display(df)\n",
    "    \n",
    "    \n",
    "gdict = pickle.load(open(f\"great-predict_shift.pickle\", 'rb'))\n",
    "dieBolder(gdict, aged=False, n_tests=315)\n",
    "\n",
    "gdict = pickle.load(open(f\"great-predict_risk.pickle\", 'rb'))\n",
    "dieBolder(gdict, aged=True, n_tests=315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a56e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diebold-Mariano plot generator\n",
    "def diePlotter(df, aged=False, thresh=0.95):\n",
    "    if aged:\n",
    "        daytimes = ['low', 'medium', 'high']\n",
    "        prefix = 'risk'\n",
    "        colors = ['xkcd:jade green', 'xkcd:tangerine', 'xkcd:scarlet']\n",
    "    else:\n",
    "        daytimes = DAYTIMES\n",
    "        prefix = 'shift'\n",
    "        colors = ['xkcd:kermit green', 'xkcd:scarlet', 'xkcd:sapphire']\n",
    "    figa, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    metcoordict = {'SARIMA-1': 0, 'SARIMA-7': 1, 'SARIMA-14': 2,\n",
    "                   'RF': 3, 'SVR': 7, 'FNN': 11}\n",
    "    varcoordict = dict(zip(['nan', 'All', 'No T', 'No W', 'No W T'], [0, *range(4)]))\n",
    "    colordict = dict(zip(daytimes, colors))\n",
    "    angledict = dict(zip(daytimes, [np.pi/6, np.pi*5/6, np.pi*3/2]))\n",
    "    varcoordict[''] = 0\n",
    "    varlabs = ['-All', '-No T', '-No W', '-No W T']\n",
    "    axlabels = ['SARIMA-1', 'SARIMA-7', 'SARIMA-14',\n",
    "                *[''.join([m, v]) for m in ['RF', 'SVR', 'FNN'] for v in varlabs]]\n",
    "    for _, line in df.iterrows():\n",
    "        x = metcoordict[line['method_1']] + varcoordict[str(line['input_1'])]\n",
    "        y = metcoordict[line['method_2']] + varcoordict[str(line['input_2'])]\n",
    "        if line['shift'].lower() == 'morning' or line['shift'].lower() == 'low':\n",
    "            ax.scatter(x, y, color='white', edgecolor='black', zorder=9, s=400)\n",
    "        if line['equiv_frac'] > thresh:\n",
    "            ax.scatter(x + np.cos(angledict[line['shift'].lower()])*0.15,\n",
    "                       y + np.sin(angledict[line['shift'].lower()])*0.15,\n",
    "                       color=colordict[line['shift'].lower()], zorder=10,\n",
    "                       edgecolor='black', s=50)\n",
    "    for t in daytimes:\n",
    "        ax.scatter([], [], color=colordict[t], zorder=10, edgecolor='black', s=50, label=t)\n",
    "    ax.set_xticks(range(0, 15))\n",
    "    ax.set_yticks(range(0, 15))\n",
    "    ax.set_xticklabels(axlabels, rotation=90)\n",
    "    ax.set_yticklabels(axlabels)\n",
    "    ax.legend(loc='lower right', title='The model is equivalent\\nwhen predicting:')\n",
    "    ax.set_xlim(-1, 15)\n",
    "    ax.set_ylim(-1, 15)\n",
    "    plt.savefig(f\"diebold-plot_{prefix}.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "for aged, prefix in [[False, 'shift'], [True, 'risk']]:\n",
    "    die_df = pd.read_csv(f\"diebold-table_{prefix}.csv\")\n",
    "    diePlotter(die_df, aged=aged, thresh=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a0427",
   "metadata": {},
   "source": [
    "### Generating Figures 5 and 6\n",
    "We chose RF (No W.) as the optimal model, but the function can be adapted to any method and input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7340c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance function\n",
    "def perforMeter(pred_y, true_y, shifts=DAYTIMES):\n",
    "    diff = pred_y - true_y\n",
    "    mae = np.mean(np.abs(diff), axis=0)\n",
    "    rmse = np.sqrt(np.mean((diff) ** 2, axis=0))\n",
    "    mape = 200 * np.mean(np.abs(diff) / (np.abs(pred_y) + np.abs(true_y)), axis=0)\n",
    "    dicto = {'mape': {}, 'rmse': {}, 'mae': {}}\n",
    "    for i, dt in enumerate(shifts):\n",
    "        dicto['mape'][dt] = mape[i]\n",
    "        dicto['rmse'][dt] = rmse[i]\n",
    "        dicto['mae'][dt] = mae[i]\n",
    "    return dicto\n",
    "\n",
    "\n",
    "# Testing the model on the test dataset without bootstrap\n",
    "def timeLiner(X_train, X_test, Y_train, Y_test, aged=False, xres=7, parts=5, ntrees=100):\n",
    "    tlpath = f\"timelines_rf-opt/\"\n",
    "    \n",
    "    if not os.path.exists(tlpath):\n",
    "        os.makedirs(tlpath)\n",
    "    if aged:\n",
    "        daytimes = ['low', 'medium', 'high']\n",
    "        colors = ['xkcd:jade green', 'xkcd:tangerine', 'xkcd:scarlet']\n",
    "        prefix = 'risk'\n",
    "        ylim = 350\n",
    "    else:\n",
    "        daytimes = DAYTIMES\n",
    "        colors = ['xkcd:kermit green', 'xkcd:scarlet', 'xkcd:sapphire']\n",
    "        prefix = 'shift'\n",
    "        ylim = 275\n",
    "    cols = ['timestep', 'weekday', 'yearday', 'holiday_-2',\n",
    "           'holiday_-1', 'holiday_0', 'holiday_+1', 'holiday_+2',\n",
    "           'resident_pop', 'tourist_pop']\n",
    "    meth = 'RF-No W'\n",
    "    xaxis = X_test['timestep'].values\n",
    "    nx = xaxis.shape[0]\n",
    "    Y_train, Y_test = Y_train.values, Y_test.values\n",
    "    X_train = X_train[cols].values\n",
    "    X_test = X_test[cols].values\n",
    "    normies = [StandardScaler() for _ in range(X_train.shape[1])]\n",
    "    for i in range(len(normies)):\n",
    "        normies[i].fit(X_train[:, i].reshape(-1, 1))\n",
    "        X_train[:, i] = normies[i].transform(X_train[:, i].reshape(-1, 1)).flatten()\n",
    "        X_test[:, i] = normies[i].transform(X_test[:, i].reshape(-1, 1)).flatten()\n",
    "    woods = RandomForestRegressor(n_estimators=ntrees, n_jobs=-1) \n",
    "    woods.fit(X_train, Y_train)\n",
    "    Y_pred = woods.predict(X_test)\n",
    "    rmse = perforMeter(Y_pred, Y_test, shifts=daytimes)['rmse']\n",
    "    for part in range(1, parts + 1):\n",
    "        figa, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        for ci in range(3):\n",
    "            ax.scatter(xaxis, Y_test[:, ci], color=colors[ci], linewidth=0.1,\n",
    "                       edgecolor='black', label=daytimes[ci].capitalize(), zorder=25)\n",
    "            for y in range(Y_test.shape[0]):\n",
    "                ax.fill_between([xaxis[y] - 0.25, xaxis[y] + 0.25],\n",
    "                                Y_pred[y, ci] - rmse[daytimes[ci]],\n",
    "                                Y_pred[y, ci] + rmse[daytimes[ci]],\n",
    "                                color=colors[ci], alpha=0.33, edgecolor='black')\n",
    "        ticks = np.arange(min(xaxis), max(xaxis) + 1, xres)\n",
    "        tlabs = [datetime.strftime(ZERO_DATE + timedelta(days=int(t)),\n",
    "                                   f\"%d/%m\\n%Y\") for t in ticks]   \n",
    "        ax.set_xticks(xaxis, minor=True)\n",
    "        ax.set_xticks(ticks)\n",
    "        ax.set_xticklabels(tlabs, fontsize=13, rotation=45)\n",
    "        ax.set_ylabel('Number of incoming patients (NIP)', fontsize=13)\n",
    "        ax.set_xlabel('Time (d/m/y)', fontsize=13)\n",
    "        ax.set_xlim(xaxis[0] + (nx * (part - 1) / parts), xaxis[0] + (nx * part / parts))\n",
    "        ax.set_title(f\"Predicted vs Real NIP - {meth}\")\n",
    "        ax.legend(fontsize=14, loc='upper center', ncol=3, columnspacing=1)\n",
    "        ax.set_yticks(np.arange(0, ylim + 1, 50))\n",
    "        ax.set_yticks(np.arange(0, ylim + 1, 25), minor=True)\n",
    "        ax.set_yticklabels(np.arange(0, ylim + 1, 50), fontsize=13)\n",
    "        ax.set_ylim(0, ylim)\n",
    "        plt.savefig(f\"{tlpath}tl_{prefix}_ pt-{part}.pdf\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close(figa)\n",
    "    \n",
    "\n",
    "# shift-based predictions\n",
    "ycols = ['total_morning', 'total_afternoon', 'total_night']\n",
    "timeLiner(X_TRAIN, X_TEST, Y_TRAIN[ycols], Y_TEST[ycols], aged=False, xres=7, parts=6)\n",
    "\n",
    "# risk-group-based predictions\n",
    "ycols = ['total_low', 'total_medium', 'total_high']\n",
    "timeLiner(X_TRAIN, X_TEST, Y_TRAIN[ycols], Y_TEST[ycols], aged=True, xres=7, parts=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7c4c7",
   "metadata": {},
   "source": [
    "### Testing the models on post-COVID data\n",
    "As explained in the paper, we only test SARIMA and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d750767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance function\n",
    "def perforMeter(pred_y, true_y, shifts=DAYTIMES):\n",
    "    dicto = {'mape': {}, 'mape_std': {}, 'rmse': {},\n",
    "             'rmse_std': {}, 'mae': {}, 'mae_std': {}}\n",
    "    for t in shifts:\n",
    "        diff = pred_y[t] - true_y[t]\n",
    "        mape = 200 * np.mean(np.abs(diff) / (np.abs(pred_y[t]) + np.abs(true_y[t])), axis=1)\n",
    "        rmse = np.sqrt(np.mean((diff) ** 2, axis=1))\n",
    "        mae = np.mean(np.abs(diff), axis=1)\n",
    "        dicto['mape'][t] = np.mean(mape)\n",
    "        dicto['mape_std'][t] = np.std(mape)\n",
    "        dicto['rmse'][t] = np.mean(rmse)\n",
    "        dicto['rmse_std'][t] = np.std(rmse)\n",
    "        dicto['mae'][t] = np.mean(mae)\n",
    "        dicto['mae_std'][t] = np.std(mae)\n",
    "    return dicto\n",
    "\n",
    "\n",
    "# diebold fraction function\n",
    "def dieboFunc(pred_a, pred_b, y_test, shifts=DAYTIMES, thresh=0.05, n_tests=1):\n",
    "    diebolds = {}\n",
    "    corr = thresh / n_tests\n",
    "    for t in shifts:\n",
    "        diebolds[t] = [dm_test(y_test[t][ni], pred_a[t][ni], pred_b[t][ni])[1]\n",
    "                       for ni in range(y_test[t].shape[0])]\n",
    "        diebolds[t] = [dm > corr for dm in diebolds[t]]\n",
    "        diebolds[t] = np.sum(diebolds[t]) / len(diebolds[t])\n",
    "    return diebolds\n",
    "\n",
    "\n",
    "# Testing the models, generating the tables, and DM testing\n",
    "def theCovidPredictor(X_train, X_test, Y_train, Y_test, ntrees,\n",
    "                      niters=100, aged=False, arishift=28):\n",
    "    T = {}\n",
    "    wp_cols = ['timestep', 'weekday', 'yearday', 'holiday_-2',\n",
    "               'holiday_-1', 'holiday_0', 'holiday_+1', 'holiday_+2',\n",
    "               'temp_min', 'temp_max', 'prec_prob', 'wind_speed',\n",
    "               'resident_pop', 'tourist_pop']\n",
    "    wx_cols = ['timestep', 'weekday', 'yearday', 'holiday_-2',\n",
    "               'holiday_-1', 'holiday_0', 'holiday_+1', 'holiday_+2',\n",
    "               'temp_min', 'temp_max', 'prec_prob', 'wind_speed', 'resident_pop']\n",
    "    xp_cols = ['timestep', 'weekday', 'yearday', 'holiday_-2',\n",
    "               'holiday_-1', 'holiday_0', 'holiday_+1', 'holiday_+2',\n",
    "               'resident_pop', 'tourist_pop']\n",
    "    xx_cols = ['timestep', 'weekday', 'yearday', 'holiday_-2',\n",
    "               'holiday_-1', 'holiday_0', 'holiday_+1', 'holiday_+2', 'resident_pop']\n",
    "    if aged:\n",
    "        daytimes = ['low', 'medium', 'high']\n",
    "        prefix = 'risk'\n",
    "    else:\n",
    "        daytimes = DAYTIMES\n",
    "        prefix = 'shift'\n",
    "    # SARIMA\n",
    "    Y_train, Y_test = Y_train.values, Y_test.values\n",
    "    Y_a1 = np.zeros((len(Y_test), 3))\n",
    "    Y_a7 = np.zeros((len(Y_test), 3))\n",
    "    Y_a14 = np.zeros((len(Y_test), 3))\n",
    "    for t in tqdm(np.arange(-14 + arishift, len(Y_test)-1), position=0, leave=True,\n",
    "                  colour='red', ncols=111, desc='SARIMA'):\n",
    "        Y = Y_test[:t+1]\n",
    "        with warnings.catch_warnings(action=\"ignore\"):\n",
    "            arima = [ARIMA(Y[:, i], order=(1, 1, 1), seasonal_order=(1, 1, 1, 7),\n",
    "                           ).fit() for i in range(3)]\n",
    "        if t < len(Y_test) - 14:\n",
    "            apred = np.array([arima[i].forecast(steps=14) for i in range(3)])\n",
    "            if t > -2 + arishift:\n",
    "                Y_a1[t+1] = apred[:, 0]\n",
    "            if t > -8 + arishift:\n",
    "                Y_a7[t+7] = apred[:, 6]\n",
    "            Y_a14[t+14] = apred[:, 13]\n",
    "        if t < len(Y_test) - 7:\n",
    "            apred = np.array([arima[i].forecast(steps=7) for i in range(3)])\n",
    "            Y_a1[t+1] = apred[:, 0]\n",
    "            Y_a7[t+7] = apred[:, 6]\n",
    "        else:\n",
    "            apred = np.array([arima[i].forecast(steps=1)[0] for i in range(3)])\n",
    "            Y_a1[t+1] = apred\n",
    "    T['SARIMA-1'] = dict(zip(daytimes, np.array([Y_a1[:, t] for t in range(3)])))\n",
    "    T['SARIMA-7'] = dict(zip(daytimes, np.array([Y_a7[:, t] for t in range(3)])))\n",
    "    T['SARIMA-14'] = dict(zip(daytimes, np.array([Y_a14[:, t] for t in range(3)])))\n",
    "    # RF\n",
    "    X_tr = {'CWP': X_train[wp_cols].copy().values, 'CWX': X_train[wx_cols].copy().values,\n",
    "            'CXP': X_train[xp_cols].copy().values, 'CXX': X_train[xx_cols].copy().values}\n",
    "    T['RF'] = {}\n",
    "    normies, woods = {}, {}\n",
    "    for k, cols in zip(X_tr, [wp_cols, wx_cols, xp_cols, xx_cols]):\n",
    "        normies[k] = [StandardScaler() for _ in range(X_tr[k].shape[1])]\n",
    "        for i in range(len(normies[k])):\n",
    "            normies[k][i].fit(X_tr[k][:, i].reshape(-1, 1))\n",
    "            X_tr[k][:, i] = normies[k][i].transform(X_tr[k][:, i].reshape(-1, 1)).flatten()\n",
    "        woods[k] = RandomForestRegressor(n_estimators=ntrees, n_jobs=-1) \n",
    "        woods[k].fit(X_tr[k], Y_train)\n",
    "        T['RF'][k] = {}\n",
    "        X_te = X_test[cols].copy().values\n",
    "        for i in range(len(normies[k])):\n",
    "            X_te[:, i] = normies[k][i].transform(X_te[:, i].reshape(-1, 1)).flatten()\n",
    "        tree_preds = woods[k].predict(X_te)\n",
    "        for t, dt in enumerate(daytimes):\n",
    "            T['RF'][k][dt] = tree_preds[:, t]\n",
    "    # Bootstrap procedure\n",
    "    D = {'Y': dict(zip(daytimes, [[], [], []])),\n",
    "         'SARIMA-1': dict(zip(daytimes, [[], [], []])),\n",
    "         'SARIMA-7': dict(zip(daytimes, [[], [], []])),\n",
    "         'SARIMA-14': dict(zip(daytimes, [[], [], []])),\n",
    "         'RF': dict(zip(X_tr, [dict(zip(daytimes, [[], [], []])) for _ in range(4)])),}\n",
    "    for ni in tqdm(range(niters), colour='red', ncols=111, desc='Bootstrapping'):\n",
    "        if ni % 10 == 0:\n",
    "            np.random.seed(SEED + ni // 10)\n",
    "        vidx = np.random.randint(low=0, high=X_test.shape[0], size=X_test.shape[0])\n",
    "        for t, dt in enumerate(daytimes):\n",
    "            D['Y'][dt].append(Y_test[vidx, t])\n",
    "            D['SARIMA-1'][dt].append(T['SARIMA-1'][dt][vidx])\n",
    "            D['SARIMA-7'][dt].append(T['SARIMA-7'][dt][vidx])\n",
    "            D['SARIMA-14'][dt].append(T['SARIMA-14'][dt][vidx])\n",
    "            for k in X_tr:\n",
    "                D['RF'][k][dt].append(T['RF'][k][dt][vidx])\n",
    "    for t, dt in enumerate(daytimes):\n",
    "        for meth in ['Y', 'SARIMA-1', 'SARIMA-7', 'SARIMA-14']:\n",
    "            D[meth][dt] = np.array(D[meth][dt])\n",
    "        for k in X_tr:\n",
    "            D['RF'][k][dt] = np.array(D['RF'][k][dt])\n",
    "    niters = len(D['SARIMA-1'])\n",
    "    metrics = ['mape', 'rmse', 'mae']\n",
    "    inpudict = {'CWP': 'All', 'CXP': 'No W', 'CWX': 'No T', 'CXX': 'No W T'}\n",
    "    columns = ['method', 'input', 'shift', 'mape', 'mape_std',\n",
    "               'rmse', 'rmse_std', 'mae', 'mae_std']\n",
    "    # performance table\n",
    "    df = []\n",
    "    for meth in ['SARIMA-1', 'SARIMA-7', 'SARIMA-14']:\n",
    "        dicto = perforMeter(D[meth], D['Y'], shifts=daytimes)\n",
    "        for t in daytimes:\n",
    "            df.append([meth, '', t.capitalize(), dicto['mape'][t], dicto['mape_std'][t],\n",
    "                       dicto['rmse'][t], dicto['rmse_std'][t], dicto['mae'][t],\n",
    "                       dicto['mae_std'][t]])\n",
    "    for ind in inpudict:\n",
    "        dicto = perforMeter(D['RF'][ind], D['Y'], shifts=daytimes)\n",
    "        for t in daytimes:\n",
    "            df.append(['RF', inpudict[ind], t.capitalize(), dicto['mape'][t],\n",
    "                       dicto['mape_std'][t], dicto['rmse'][t], dicto['rmse_std'][t],\n",
    "                       dicto['mae'][t], dicto['mae_std'][t]])\n",
    "    df = pd.DataFrame(df, columns=columns)\n",
    "    df.to_csv(f\"covid-table-full_{prefix}.csv\", index=None)\n",
    "    display(df)\n",
    "    # diebold Covid\n",
    "    n_tests = 63\n",
    "    dbar = tqdm(total=n_tests, colour='green', ncols=111,\n",
    "                position=0, leave=True, desc=f'die-{prefix}')\n",
    "    df = []\n",
    "    columns = ['method_1', 'input_1', 'method_2', 'input_2', 'shift', 'equiv_frac']\n",
    "    for m1, m2 in combinations(['SARIMA-1', 'SARIMA-7', 'SARIMA-14'], 2):\n",
    "        diebold = dieboFunc(D[m1], D[m2], D['Y'],\n",
    "                            shifts=daytimes, thresh=0.05, n_tests=n_tests)\n",
    "        dbar.update(3)\n",
    "        for t in daytimes:\n",
    "            df.append([m1, '', m2, '', t.capitalize(), diebold[t]])\n",
    "    for bmeth in ['SARIMA-1', 'SARIMA-7', 'SARIMA-14']:\n",
    "        for ind in inpudict:\n",
    "            diebold = dieboFunc(D[bmeth], D['RF'][ind], D['Y'],\n",
    "                                shifts=daytimes, thresh=0.05, n_tests=n_tests)\n",
    "            dbar.update(3)\n",
    "            for t in daytimes:\n",
    "                df.append([bmeth, '', 'RF', inpudict[ind], t.capitalize(), diebold[t]])\n",
    "    for v1, v2 in combinations(['CWP', 'CWX', 'CXP', 'CXX'], 2):\n",
    "        diebold = dieboFunc(D['RF'][v1], D['RF'][v2], D['Y'],\n",
    "                            shifts=daytimes, thresh=0.05, n_tests=n_tests)\n",
    "        dbar.update(3)\n",
    "        for t in daytimes:\n",
    "            df.append(['RF', inpudict[v1], 'RF', inpudict[v2], t.capitalize(), diebold[t]])\n",
    "    dbar.close()\n",
    "    df = pd.DataFrame(df, columns=columns)\n",
    "    df.to_csv(f\"covid-diebold_{prefix}.csv\", index=None)\n",
    "    display(df)\n",
    "    \n",
    "    \n",
    "# shift-based predictions\n",
    "ycols = ['total_morning', 'total_afternoon', 'total_night']\n",
    "theCovidPredictor(X_TRAIN, X_COVID, Y_TRAIN[ycols], Y_COVID[ycols],\n",
    "                  ntrees=100, niters=1000, aged=False)\n",
    "\n",
    "# risk-group-based predictions\n",
    "ycols = ['total_low', 'total_medium', 'total_high']\n",
    "theCovidPredictor(X_TRAIN, X_COVID, Y_TRAIN[ycols], Y_COVID[ycols],\n",
    "                  ntrees=100, niters=1000, aged=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
